import java.io.File
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.hive.HiveContext
val hiveContext = new HiveContext(sc)
val sparkSession = SparkSession.builder()
 .master("spark://spark-master:7077")
 .appName("files to hive")
 .config("spark.sql.warehouse.dir", "/user/hive/warehouse")
 .enableHiveSupport()
 .getOrCreate()
// val warehouseLocation = new File("spark-warehouse").getAbsolutePath
// println(warehouseLocation)
spark.sql("CREATE DATABASE IF NOT EXISTS db_nubank")
spark.sql("USE db_nubank")
:paste
import org.apache.hadoop.fs._
var df_accounts = spark.read.format("csv").option("header", "true").load("file:///dataset/nubank/accounts/")
df_accounts.printSchema()
df_accounts = df_accounts.selectExpr("account_id"
                      ,"customer_id"
                      ,"cast(created_at as timestamp)"
                      ,"status"
                      ,"account_branch"
                      ,"account_check_digit"
                      ,"account_number")
df_accounts.show(2,false)
df_accounts.toDF.write
  .mode("Overwrite")
  .saveAsTable("db_nubank.accounts")
:paste
df_accounts.toDF.write
  .mode("Overwrite")
  .saveAsTable("db_nubank.accounts")
