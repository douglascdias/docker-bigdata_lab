{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tutorial-syndicate",
   "metadata": {},
   "source": [
    "# Create datasets e import in Hive - transfer_monthly_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-french",
   "metadata": {},
   "source": [
    "## Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaging-simulation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://8ea9f1e3afef:4040\n",
       "SparkContext available as 'sc' (version = 3.0.2, master = local[*], app id = local-1615150323907)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import java.io.File\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.hive.HiveContext\n",
       "hiveContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@3b5516a0\n",
       "warehouseLocation: String = hdfs://namenode:8020/user/hive/warehouse\n",
       "data_base: String = db_nubank\n",
       "table: String = \"\"\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/\n",
       "db_table: String = db_nubank.\n",
       "res0: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import java.io.File\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.hive.HiveContext\n",
    "\n",
    "val hiveContext = new HiveContext(sc)\n",
    "val warehouseLocation = \"hdfs://namenode:8020/user/hive/warehouse\"\n",
    "val data_base = \"db_nubank\"\n",
    "var table = \"\"\n",
    "var dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "var db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"use db_nubank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-vitamin",
   "metadata": {},
   "source": [
    "## Import data to datasets (transfer tables) and union all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becoming-purple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sql_transfer_ins: String =\n",
       "select  t.id,t.account_id,t.status,t.amount\n",
       ",time.action_timestamp, mon.action_month, year.action_year\n",
       ",0 flag_is_pix\n",
       "from transfer_ins t\n",
       "inner join d_time time\n",
       "on t.transaction_completed_at = time.time_id\n",
       "inner join d_month mon\n",
       "on mon.month_id = time.month_id\n",
       "inner join d_year year\n",
       "on year.year_id = time.year_id\n",
       "sql_transfer_outs: String =\n",
       "select t.id,t.account_id,t.status,t.amount*-1\n",
       ",time.action_timestamp, mon.action_month, year.action_year\n",
       ",0 flag_is_pix\n",
       "from transfer_outs t\n",
       "inner join d_time time\n",
       "on t.transaction_completed_at = time.time_id\n",
       "inner join d_month mon\n",
       "on mon.month_id = time.month_id\n",
       "inner join d_year year\n",
       "on year.year_id = time.year_id\n",
       "sql_pix_movements: String =\n",
       "select t.id,t.account_id,t.status\n",
       ",case when t.in_or_out = 'pix_out' then t.pix_a...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sql_transfer_ins = \"\"\"select  t.id,t.account_id,t.status,t.amount\n",
    ",time.action_timestamp, mon.action_month, year.action_year\n",
    ",0 flag_is_pix\n",
    "from transfer_ins t\n",
    "inner join d_time time\n",
    "on t.transaction_completed_at = time.time_id\n",
    "inner join d_month mon\n",
    "on mon.month_id = time.month_id\n",
    "inner join d_year year\n",
    "on year.year_id = time.year_id\"\"\"\n",
    "\n",
    "val sql_transfer_outs = \"\"\"select t.id,t.account_id,t.status,t.amount*-1\n",
    ",time.action_timestamp, mon.action_month, year.action_year\n",
    ",0 flag_is_pix\n",
    "from transfer_outs t\n",
    "inner join d_time time\n",
    "on t.transaction_completed_at = time.time_id\n",
    "inner join d_month mon\n",
    "on mon.month_id = time.month_id\n",
    "inner join d_year year\n",
    "on year.year_id = time.year_id\"\"\"\n",
    "\n",
    "val sql_pix_movements = \"\"\"select t.id,t.account_id,t.status\n",
    ",case when t.in_or_out = 'pix_out' then t.pix_amount*-1\n",
    "  else t.pix_amount end as amount, time.action_timestamp, mon.action_month, year.action_year\n",
    ",1 flag_is_pix\n",
    "from pix_movements t\n",
    "inner join d_time time\n",
    "on t.pix_completed_at = time.time_id\n",
    "inner join d_month mon\n",
    "on mon.month_id = time.month_id\n",
    "inner join d_year year\n",
    "on year.year_id = time.year_id\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val df_transfer_ins = spark.sql(sql_transfer_ins)\n",
    "val df_transfer_outs = spark.sql(sql_transfer_outs)\n",
    "val df_pix_movements = spark.sql(sql_pix_movements)\n",
    "var df_transfer_union = df_transfer_ins.union(df_transfer_outs.union(df_pix_movements))\n",
    "\n",
    "// Test accounts without transfer on month 2\n",
    "// df_transfer_union = df_transfer_union.where(!($\"action_month\" === 2))\n",
    "\n",
    "df_transfer_union.createOrReplaceTempView(\"tmp_transfer_union\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-bacteria",
   "metadata": {},
   "source": [
    "## Includes months without transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sticky-miracle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sql_accounts_month_year: String =\n",
       "\"select * from accounts,d_month,d_year\n",
       "                     where concat(cast(year(created_at) as string),lpad(cast(month(created_at) as string),2,'0'))\n",
       "                           <= concat(cast(action_year as string),lpad(cast(action_month as string),2,'0'))\n",
       "                    \"\n",
       "df_accounts_month_year: org.apache.spark.sql.DataFrame = [account_id: string, customer_id: string ... 9 more fields]\n",
       "sql_transfer: String =\n",
       "\"select\n",
       "                      ac.account_id\n",
       "                     ,ac.account_branch\n",
       "                     ,ac.account_check_digit\n",
       "                     ,ac.account_number\n",
       "                     ,coalesce(tr.amount,0) amount\n",
       "                     ,coalesce(tr.action_timestamp\n",
       "                          ,cast(\n",
       "                              concat(...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sql_accounts_month_year = \"\"\"select * from accounts,d_month,d_year\n",
    "                     where concat(cast(year(created_at) as string),lpad(cast(month(created_at) as string),2,'0'))\n",
    "                           <= concat(cast(action_year as string),lpad(cast(action_month as string),2,'0'))\n",
    "                    \"\"\"\n",
    "val df_accounts_month_year = spark.sql(sql_accounts_month_year)\n",
    "df_accounts_month_year.createOrReplaceTempView(\"tmp_accounts\")\n",
    "\n",
    "val sql_transfer = \"\"\"select \n",
    "                      ac.account_id\n",
    "                     ,ac.account_branch\n",
    "                     ,ac.account_check_digit\n",
    "                     ,ac.account_number\n",
    "                     ,coalesce(tr.amount,0) amount\n",
    "                     ,coalesce(tr.action_timestamp\n",
    "                          ,cast(\n",
    "                              concat(cast(ac.action_year as string)\n",
    "                                    ,'-'\n",
    "                                    ,lpad(cast(ac.action_month as string),2,'0')\n",
    "                                    ,'-01')                             \n",
    "                                    as timestamp)\n",
    "                              ) as action_timestamp\n",
    "                     ,ac.action_month\n",
    "                     ,ac.action_year\n",
    "                     ,tr.flag_is_pix\n",
    "                     ,coalesce(tr.status,\"completed\") status\n",
    "                    from tmp_accounts ac\n",
    "                    left join tmp_transfer_union tr\n",
    "                    on ac.account_id = tr.account_id\n",
    "                    and ac.action_month = tr.action_month\n",
    "                    and ac.action_year = tr.action_year\n",
    "                    \"\"\"\n",
    "\n",
    "val df_transfer = spark.sql(sql_transfer)\n",
    "df_transfer.createOrReplaceTempView(\"tmp_transfer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-energy",
   "metadata": {},
   "source": [
    "## Calculates amount_balance column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "champion-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- account_branch: string (nullable = true)\n",
      " |-- account_check_digit: string (nullable = true)\n",
      " |-- account_number: string (nullable = true)\n",
      " |-- amount: float (nullable = false)\n",
      " |-- action_timestamp: timestamp (nullable = true)\n",
      " |-- action_month: integer (nullable = true)\n",
      " |-- action_year: string (nullable = true)\n",
      " |-- flag_is_pix: integer (nullable = true)\n",
      " |-- status: string (nullable = false)\n",
      " |-- amount_balance: double (nullable = true)\n",
      "\n",
      "+-------------------+--------------+-------------------+--------------+--------+----------------------+------------+-----------+-----------+---------+--------------+\n",
      "|account_id         |account_branch|account_check_digit|account_number|amount  |action_timestamp      |action_month|action_year|flag_is_pix|status   |amount_balance|\n",
      "+-------------------+--------------+-------------------+--------------+--------+----------------------+------------+-----------+-----------+---------+--------------+\n",
      "|1029774162761347456|3748          |4                  |73201         |-1407.71|2020-02-11 17:56:54.67|2           |2020       |1          |completed|477.2         |\n",
      "|1029774162761347456|3748          |4                  |73201         |-1801.93|2020-08-11 09:28:36.18|8           |2020       |0          |completed|-880.71       |\n",
      "|1029774162761347456|3748          |4                  |73201         |1304.84 |2020-08-14 12:05:38.67|8           |2020       |1          |completed|424.13        |\n",
      "|1029774162761347456|3748          |4                  |73201         |-1504.84|2020-12-05 08:08:32.15|12          |2020       |0          |completed|3470.95       |\n",
      "|1029774162761347456|3748          |4                  |73201         |-360.69 |2020-12-09 19:11:02.68|12          |2020       |1          |completed|3110.26       |\n",
      "|1029774162761347456|3748          |4                  |73201         |-764.53 |2020-05-12 13:53:51.73|5           |2020       |0          |completed|276.48        |\n",
      "|1029774162761347456|3748          |4                  |73201         |1747.36 |2020-05-11 14:27:17.61|5           |2020       |1          |completed|1041.01       |\n",
      "|1029774162761347456|3748          |4                  |73201         |644.74  |2020-06-24 06:58:27.98|6           |2020       |1          |completed|921.22        |\n",
      "|1029774162761347456|3748          |4                  |73201         |527.71  |2020-03-15 13:47:53.89|3           |2020       |0          |completed|-706.35       |\n",
      "|1029774162761347456|3748          |4                  |73201         |-1711.26|2020-03-06 19:25:36.42|3           |2020       |0          |completed|-1234.06      |\n",
      "+-------------------+--------------+-------------------+--------------+--------+----------------------+------------+-----------+-----------+---------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sql_balance: String =\n",
       "\"\n",
       " select\n",
       "  t1.account_id\n",
       " ,t1.account_branch\n",
       " ,t1.account_check_digit\n",
       " ,t1.account_number\n",
       " ,t1.amount\n",
       " ,t1.action_timestamp\n",
       " ,t1.action_month\n",
       " ,t1.action_year\n",
       " ,t1.flag_is_pix\n",
       " ,t1.status\n",
       " ,round(sum(t2.amount),2) amount_balance\n",
       " from tmp_transfer as t1\n",
       " inner join tmp_transfer as t2\n",
       " ON t1.action_timestamp >= t2.action_timestamp\n",
       " and t1.account_id = t2.account_id\n",
       " WHERE t1.status = \"completed\"\n",
       " GROUP BY\n",
       "  t1.account_id\n",
       " ,t1.account_branch\n",
       " ,t1.account_check_digit\n",
       " ,t1.account_number\n",
       " ,t1.amount\n",
       " ,t1.action_timestamp\n",
       " ,t1.action_month\n",
       " ,t1.action_year\n",
       " ,t1.flag_is_pix\n",
       " ,t1.status\n",
       "\"\n",
       "df_transfer_balance: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [account_id: string, account_branch: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sql_balance = \"\"\"\n",
    " select\n",
    "  t1.account_id\n",
    " ,t1.account_branch\n",
    " ,t1.account_check_digit\n",
    " ,t1.account_number\n",
    " ,t1.amount\n",
    " ,t1.action_timestamp\n",
    " ,t1.action_month\n",
    " ,t1.action_year\n",
    " ,t1.flag_is_pix\n",
    " ,t1.status\n",
    " ,round(sum(t2.amount),2) amount_balance\n",
    " from tmp_transfer as t1\n",
    " inner join tmp_transfer as t2\n",
    " ON t1.action_timestamp >= t2.action_timestamp\n",
    " and t1.account_id = t2.account_id\n",
    " WHERE t1.status = \"completed\"\n",
    " GROUP BY\n",
    "  t1.account_id\n",
    " ,t1.account_branch\n",
    " ,t1.account_check_digit\n",
    " ,t1.account_number\n",
    " ,t1.amount\n",
    " ,t1.action_timestamp\n",
    " ,t1.action_month\n",
    " ,t1.action_year\n",
    " ,t1.flag_is_pix\n",
    " ,t1.status\n",
    "\"\"\"\n",
    "var df_transfer_balance = spark.sql(sql_balance).persist()\n",
    "\n",
    "df_transfer_balance.printSchema()\n",
    "\n",
    "df_transfer_balance.createOrReplaceTempView(\"tmp_transfer_balance\")\n",
    "\n",
    "df_transfer_balance.show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-assembly",
   "metadata": {},
   "source": [
    "## Generates the \"Account Monthly Balance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amateur-eagle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-------------------+--------------+------------+-----------+--------------+\n",
      "|         account_id|account_branch|account_check_digit|account_number|action_month|action_year|amount_balance|\n",
      "+-------------------+--------------+-------------------+--------------+------------+-----------+--------------+\n",
      "|1000667155163612544|          1565|                  7|         52530|           1|       2020|       5159.38|\n",
      "|1000667155163612544|          1565|                  7|         52530|           2|       2020|      10677.62|\n",
      "|1000667155163612544|          1565|                  7|         52530|           3|       2020|      11662.26|\n",
      "|1000667155163612544|          1565|                  7|         52530|           4|       2020|      11585.97|\n",
      "|1000667155163612544|          1565|                  7|         52530|           5|       2020|      12496.18|\n",
      "|1000667155163612544|          1565|                  7|         52530|           6|       2020|       -549.96|\n",
      "|1000667155163612544|          1565|                  7|         52530|           7|       2020|       2961.47|\n",
      "|1000667155163612544|          1565|                  7|         52530|           8|       2020|      -6105.89|\n",
      "|1000667155163612544|          1565|                  7|         52530|           9|       2020|      -9156.81|\n",
      "|1000667155163612544|          1565|                  7|         52530|          10|       2020|     -12811.15|\n",
      "|1000667155163612544|          1565|                  7|         52530|          11|       2020|      -5495.49|\n",
      "|1000667155163612544|          1565|                  7|         52530|          12|       2020|      -6043.95|\n",
      "|1001241849719113600|          7037|                  8|         63843|           1|       2020|       1106.77|\n",
      "|1001241849719113600|          7037|                  8|         63843|           2|       2020|        784.72|\n",
      "|1001241849719113600|          7037|                  8|         63843|           3|       2020|       1594.55|\n",
      "|1001241849719113600|          7037|                  8|         63843|           4|       2020|       1594.55|\n",
      "|1001241849719113600|          7037|                  8|         63843|           5|       2020|       2395.22|\n",
      "|1001241849719113600|          7037|                  8|         63843|           6|       2020|       1748.36|\n",
      "|1001241849719113600|          7037|                  8|         63843|           7|       2020|       1748.36|\n",
      "|1001241849719113600|          7037|                  8|         63843|           8|       2020|      -2386.44|\n",
      "+-------------------+--------------+-------------------+--------------+------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sql: String =\n",
       "\"\n",
       "select\n",
       " t1.account_id\n",
       ",t1.account_branch\n",
       ",t1.account_check_digit\n",
       ",t1.account_number\n",
       ",t1.action_month\n",
       ",t1.action_year\n",
       ",t1.amount_balance\n",
       "from tmp_transfer_balance t1\n",
       "inner join\n",
       " (select account_id,action_month,action_year,max(action_timestamp) action_timestamp\n",
       "  from tmp_transfer_balance\n",
       "  group by  account_id,action_month,action_year) t2\n",
       "on t1.account_id = t2.account_id\n",
       "and t1.action_timestamp = t2.action_timestamp\n",
       "where 1=1\n",
       "--and t1.account_id = \"2481861278853952512\"\n",
       "and t1.action_year = 2020\n",
       "order by t1.account_id, action_year, action_month\n",
       "\"\n",
       "df_transfer_monthly_balance: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [account_id: string, account_branch: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sql = \"\"\"\n",
    "select   \n",
    " t1.account_id\n",
    ",t1.account_branch\n",
    ",t1.account_check_digit\n",
    ",t1.account_number\n",
    ",t1.action_month\n",
    ",t1.action_year\n",
    ",t1.amount_balance\n",
    "from tmp_transfer_balance t1\n",
    "inner join \n",
    " (select account_id,action_month,action_year,max(action_timestamp) action_timestamp \n",
    "  from tmp_transfer_balance\n",
    "  group by  account_id,action_month,action_year) t2\n",
    "on t1.account_id = t2.account_id\n",
    "and t1.action_timestamp = t2.action_timestamp\n",
    "where 1=1\n",
    "--and t1.account_id = \"2481861278853952512\" \n",
    "and t1.action_year = 2020\n",
    "order by t1.account_id, action_year, action_month\n",
    "\"\"\"\n",
    "\n",
    "val df_transfer_monthly_balance = spark.sql(sql).persist()\n",
    "df_transfer_monthly_balance.show()\n",
    "// df_transfer_monthly_balance.createOrReplaceTempView(\"tmp_transfer_monthly_balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-collector",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ultimate-lighting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "table: String = transfer_union\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/transfer_union\n",
       "db_table: String = db_nubank.transfer_union\n",
       "table: String = transfer_union\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/transfer_union\n",
       "db_table: String = db_nubank.transfer_union\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transfer_monthly_balance\n",
    "    .repartition(1)\n",
    "    .write\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"file:////dataset/nubank/out_transfer_monthly_balance\")\n",
    "\n",
    "df_transfer_union\n",
    "    .repartition(1)\n",
    "    .write\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"file:////dataset/nubank/out_transfer_union\")\n",
    "\n",
    "table = \"transfer_monthly_balance\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_transfer_monthly_balance.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)\n",
    "\n",
    "table = \"transfer_union\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_transfer_union.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(sql).explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
