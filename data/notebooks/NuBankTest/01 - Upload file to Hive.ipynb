{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tutorial-syndicate",
   "metadata": {},
   "source": [
    "# Create datasets e import in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-anime",
   "metadata": {},
   "source": [
    "## Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaging-simulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://2a03b9dad4f2:4041\n",
       "SparkContext available as 'sc' (version = 3.0.2, master = local[*], app id = local-1615077938167)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import java.io.File\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.hive.HiveContext\n",
       "hiveContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@64fdba00\n",
       "warehouseLocation: String = hdfs://namenode:8020/user/hive/warehouse\n",
       "data_base: String = db_nubank\n",
       "table: String = \"\"\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/\n",
       "db_table: String = db_nubank.\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.File\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.hive.HiveContext\n",
    "val hiveContext = new HiveContext(sc)\n",
    "\n",
    "val warehouseLocation = \"hdfs://namenode:8020/user/hive/warehouse\"\n",
    "\n",
    "// val warehouseLocation = new File(\"spark-warehouse\").getAbsolutePath\n",
    "// println(warehouseLocation)\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS db_nubank\")\n",
    "spark.sql(\"USE db_nubank\")\n",
    "\n",
    "val data_base = \"db_nubank\"\n",
    "var table = \"\"\n",
    "var dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "var db_table = data_base + \".\" + table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-vitamin",
   "metadata": {},
   "source": [
    "### Create datasets and saving in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becoming-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- account_branch: string (nullable = true)\n",
      " |-- account_check_digit: string (nullable = true)\n",
      " |-- account_number: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+--------------------+------+--------------+-------------------+--------------+\n",
      "|         account_id|        customer_id|          created_at|status|account_branch|account_check_digit|account_number|\n",
      "+-------------------+-------------------+--------------------+------+--------------+-------------------+--------------+\n",
      "| 509281836645315264|3287830764476260864|2019-11-27 22:02:...|active|          7763|                  9|         38218|\n",
      "|1464307209104691456|2739905374464312320|2019-04-02 12:42:...|active|          7183|                  2|          1684|\n",
      "+-------------------+-------------------+--------------------+------+--------------+-------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_accounts: org.apache.spark.sql.DataFrame = [account_id: string, customer_id: string ... 5 more fields]\n",
       "df_accounts: org.apache.spark.sql.DataFrame = [account_id: string, customer_id: string ... 5 more fields]\n",
       "table: String = accounts\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/accounts\n",
       "db_table: String = db_nubank.accounts\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_accounts = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/accounts/\")\n",
    "\n",
    "df_accounts.printSchema()\n",
    "\n",
    "df_accounts = df_accounts.selectExpr(\"account_id\"\n",
    "                      ,\"customer_id\"\n",
    "                      ,\"cast(created_at as timestamp)\"\n",
    "                      ,\"status\"\n",
    "                      ,\"account_branch\"\n",
    "                      ,\"account_check_digit\"\n",
    "                      ,\"account_number\")\n",
    "df_accounts.show(2,true)\n",
    "\n",
    "table = \"accounts\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_accounts.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "willing-kansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_id: string (nullable = true)\n",
      " |-- city_id: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+-------------------+\n",
      "|city               |state_id           |city_id            |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|São José do Goiabal|2755422274446512640|670134511382200832 |\n",
      "|Pederneiras        |2066774635771587840|1734898293086970880|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+-------------------+-------------------+\n",
      "|city_id            |city               |state_id           |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|670134511382200832 |São José do Goiabal|2755422274446512640|\n",
      "|1734898293086970880|Pederneiras        |2066774635771587840|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_city: org.apache.spark.sql.DataFrame = [city_id: bigint, city: string ... 1 more field]\n",
       "df_city: org.apache.spark.sql.DataFrame = [city_id: bigint, city: string ... 1 more field]\n",
       "table: String = city\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/city\n",
       "db_table: String = db_nubank.city\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_city = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/city/\")\n",
    "\n",
    "df_city.printSchema()\n",
    "\n",
    "df_city.show(2,false)\n",
    "\n",
    "df_city = df_city.selectExpr(\"cast(city_id as bigint)\"\n",
    "                      ,\"city\"\n",
    "                      ,\"state_id\")\n",
    "df_city.show(2,false)\n",
    "\n",
    "table = \"city\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_city.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worse-vitamin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- country_id: string (nullable = true)\n",
      "\n",
      "+-------+------------------+\n",
      "|country|        country_id|\n",
      "+-------+------------------+\n",
      "| Brasil|465471097668177088|\n",
      "+-------+------------------+\n",
      "\n",
      "+------------------+-------+\n",
      "|country_id        |country|\n",
      "+------------------+-------+\n",
      "|465471097668177088|Brasil |\n",
      "+------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_country: org.apache.spark.sql.DataFrame = [country_id: string, country: string]\n",
       "df_country: org.apache.spark.sql.DataFrame = [country_id: string, country: string]\n",
       "table: String = country\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/country\n",
       "db_table: String = db_nubank.country\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_country = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/country/\")\n",
    "\n",
    "df_country.printSchema()\n",
    "\n",
    "df_country.show(2)\n",
    "\n",
    "df_country = df_country.selectExpr(\"country_id\"\n",
    "                                   ,\"country\")\n",
    "df_country.show(2,false)\n",
    "\n",
    "table = \"country\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_country.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "still-jimmy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- cpf: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      "\n",
      "+-------------------+----------+---------+-------------------+-----------+------------+\n",
      "|        customer_id|first_name|last_name|      customer_city|        cpf|country_name|\n",
      "+-------------------+----------+---------+-------------------+-----------+------------+\n",
      "|3287830764476260864|     Janet|   Ritter|1512698933146656256|85974914067|      Brasil|\n",
      "|2739905374464312320|      Tina| Lamaster|2215180425815138560|78347385617|      Brasil|\n",
      "+-------------------+----------+---------+-------------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+----------+---------+-------------------+------------+-----------+\n",
      "|customer_id        |first_name|last_name|customer_city      |country_name|cpf        |\n",
      "+-------------------+----------+---------+-------------------+------------+-----------+\n",
      "|3287830764476260864|Janet     |Ritter   |1512698933146656256|Brasil      |85974914067|\n",
      "|2739905374464312320|Tina      |Lamaster |2215180425815138560|Brasil      |78347385617|\n",
      "+-------------------+----------+---------+-------------------+------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_customers: org.apache.spark.sql.DataFrame = [customer_id: string, first_name: string ... 4 more fields]\n",
       "df_customers: org.apache.spark.sql.DataFrame = [customer_id: string, first_name: string ... 4 more fields]\n",
       "table: String = customers\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/customers\n",
       "db_table: String = db_nubank.customers\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_customers = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/customers/\")\n",
    "\n",
    "df_customers.printSchema()\n",
    "\n",
    "df_customers.show(2)\n",
    "\n",
    "df_customers = df_customers.selectExpr(\"customer_id\"\n",
    "                                       ,\"first_name\"\n",
    "                                      ,\"last_name\"\n",
    "                                      ,\"cast(customer_city as bigint)\"\n",
    "                                      ,\"country_name\"\n",
    "                                      ,\"cast(cpf as bigint)\"                                      \n",
    "                                      )\n",
    "df_customers.show(2,false)\n",
    "\n",
    "table = \"customers\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_customers.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "var df_d_month = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/d_month/\")\n",
    "var df_d_time = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/d_time/\")\n",
    "var df_d_week = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/d_week/\")\n",
    "var df_d_weekday = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/d_weekday/\")\n",
    "var df_d_year = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/d_year/\")\n",
    "\n",
    "///////////////////////////////////////////////////////\n",
    "df_d_month.printSchema()\n",
    "df_d_month = df_d_month.selectExpr(\"cast(month_id as bigint)\"\n",
    "                                   ,\"cast(action_month as int)\"                                      \n",
    "                                      )\n",
    "df_d_month.show(2,false)\n",
    "table = \"d_month\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_d_month.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)\n",
    "///////////////////////////////////////////////////////\n",
    "df_d_time.printSchema()\n",
    "df_d_time = df_d_time.selectExpr(\"cast(time_id as bigint)\"\n",
    "                                 ,\"cast(action_timestamp as timestamp)\"\n",
    "                                 ,\"cast(week_id as bigint)\"\n",
    "                                 ,\"cast(month_id as bigint)\"\n",
    "                                 ,\"cast(year_id as bigint)\"\n",
    "                                 ,\"cast(weekday_id as bigint)\"\n",
    "                                      )\n",
    "df_d_time.show(2,false)\n",
    "table = \"d_time\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_d_time.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)\n",
    "///////////////////////////////////////////////////////\n",
    "df_d_week.printSchema()\n",
    "df_d_week = df_d_week.selectExpr(\"cast(week_id as bigint)\"\n",
    "                                 ,\"cast(action_week as int)\"                                 \n",
    "                                      )\n",
    "df_d_week.show(2,false)\n",
    "table = \"d_week\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_d_week.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)\n",
    "///////////////////////////////////////////////////////\n",
    "df_d_weekday.printSchema()\n",
    "df_d_weekday = df_d_weekday.selectExpr(\"cast(weekday_id as bigint)\"\n",
    "                                 ,\"cast(action_weekday as string)\"                                 \n",
    "                                      )\n",
    "df_d_weekday.show(2,false)\n",
    "table = \"d_weekday\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_d_weekday.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)\n",
    "///////////////////////////////////////////////////////\n",
    "df_d_year.printSchema()\n",
    "df_d_year = df_d_year.selectExpr(\"cast(year_id as bigint)\"\n",
    "                                 ,\"cast(action_year as string)\"                                 \n",
    "                                      )\n",
    "df_d_year.show(2,false)\n",
    "table = \"d_year\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_d_year.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "geographic-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- in_or_out: string (nullable = true)\n",
      " |-- pix_amount: string (nullable = true)\n",
      " |-- pix_requested_at: string (nullable = true)\n",
      " |-- pix_completed_at: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "+-------------------+------------------+---------+----------+----------------+----------------+---------+\n",
      "|id                 |account_id        |in_or_out|pix_amount|pix_requested_at|pix_completed_at|status   |\n",
      "+-------------------+------------------+---------+----------+----------------+----------------+---------+\n",
      "|1362907709468179968|509281836645315264|pix_out  |1894.77   |1579693633580   |1579693646070   |completed|\n",
      "|2246794118022659072|509281836645315264|pix_out  |419.79    |1587309244550   |1587309252480   |completed|\n",
      "+-------------------+------------------+---------+----------+----------------+----------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_pix_movements: org.apache.spark.sql.DataFrame = [id: string, account_id: string ... 5 more fields]\n",
       "df_pix_movements: org.apache.spark.sql.DataFrame = [id: string, account_id: string ... 5 more fields]\n",
       "table: String = pix_movements\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/pix_movements\n",
       "db_table: String = db_nubank.pix_movements\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_pix_movements = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/pix_movements/\")\n",
    "\n",
    "df_pix_movements.printSchema()\n",
    "df_pix_movements = df_pix_movements.selectExpr(\"id\"\n",
    "                                               ,\"account_id\"\n",
    "                                               ,\"in_or_out\"\n",
    "                                               ,\"cast(pix_amount as float)\"\n",
    "                                               ,\"cast(pix_requested_at as bigint)\"\n",
    "                                               ,\"cast(pix_completed_at as bigint)\"\n",
    "                                               ,\"status\"\n",
    "                                              )\n",
    "df_pix_movements.show(2,false)\n",
    "table = \"pix_movements\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_pix_movements.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "packed-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- country_id: string (nullable = true)\n",
      " |-- state_id: string (nullable = true)\n",
      "\n",
      "+-------------------+-----+------------------+\n",
      "|state_id           |state|country_id        |\n",
      "+-------------------+-----+------------------+\n",
      "|2755422274446512640|MG   |465471097668177088|\n",
      "|2066774635771587840|SP   |465471097668177088|\n",
      "+-------------------+-----+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_state: org.apache.spark.sql.DataFrame = [state_id: string, state: string ... 1 more field]\n",
       "df_state: org.apache.spark.sql.DataFrame = [state_id: string, state: string ... 1 more field]\n",
       "table: String = state\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/state\n",
       "db_table: String = db_nubank.state\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_state = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/state/\")\n",
    "\n",
    "df_state.printSchema()\n",
    "df_state = df_state.selectExpr(\"state_id\"                \n",
    "                               ,\"state\"\n",
    "                               ,\"cast(country_id as bigint)\"\n",
    "                              )\n",
    "df_state.show(2,false)\n",
    "table = \"state\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_state.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adequate-protest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- transaction_requested_at: string (nullable = true)\n",
      " |-- transaction_completed_at: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "+-------------------+------------------+-------+------------------------+------------------------+---------+\n",
      "|id                 |account_id        |amount |transaction_requested_at|transaction_completed_at|status   |\n",
      "+-------------------+------------------+-------+------------------------+------------------------+---------+\n",
      "|652457358030649600 |509281836645315264|1481.33|1579073609010           |1579073618020           |completed|\n",
      "|1622746437516706816|509281836645315264|995.36 |1595780510110           |1595780513170           |completed|\n",
      "+-------------------+------------------+-------+------------------------+------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_transfer_ins: org.apache.spark.sql.DataFrame = [id: string, account_id: string ... 4 more fields]\n",
       "df_transfer_ins: org.apache.spark.sql.DataFrame = [id: string, account_id: string ... 4 more fields]\n",
       "table: String = transfer_ins\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/transfer_ins\n",
       "db_table: String = db_nubank.transfer_ins\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_transfer_ins = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/transfer_ins/\")\n",
    "df_transfer_ins.printSchema()\n",
    "df_transfer_ins = df_transfer_ins.selectExpr(\"id\"                \n",
    "                                             ,\"account_id\"\n",
    "                                             ,\"cast(amount as float)\"\n",
    "                                             ,\"cast(transaction_requested_at as bigint)\"\n",
    "                                             ,\"cast(transaction_completed_at as bigint)\"\n",
    "                                             ,\"status\"\n",
    "                              )\n",
    "df_transfer_ins.show(2,false)\n",
    "table = \"transfer_ins\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_transfer_ins.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "enclosed-clearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- transaction_requested_at: string (nullable = true)\n",
      " |-- transaction_completed_at: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "+-------------------+------------------+-------+------------------------+------------------------+---------+\n",
      "|id                 |account_id        |amount |transaction_requested_at|transaction_completed_at|status   |\n",
      "+-------------------+------------------+-------+------------------------+------------------------+---------+\n",
      "|327917790912071680 |509281836645315264|1794.11|1578150053870           |1578150055960           |completed|\n",
      "|3029924863588401664|509281836645315264|1197.53|1602776874060           |1602776879440           |completed|\n",
      "+-------------------+------------------+-------+------------------------+------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_transfer_outs: org.apache.spark.sql.DataFrame = [id: string, account_id: string ... 4 more fields]\n",
       "df_transfer_outs: org.apache.spark.sql.DataFrame = [id: string, account_id: string ... 4 more fields]\n",
       "table: String = transfer_outs\n",
       "dir_path: String = hdfs://namenode:8020/user/hive/warehouse/db_nubank.db/transfer_outs\n",
       "db_table: String = db_nubank.transfer_outs\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_transfer_outs = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///dataset/nubank/transfer_outs/\")\n",
    "df_transfer_outs.printSchema()\n",
    "df_transfer_outs = df_transfer_outs.selectExpr(\"id\"                \n",
    "                                             ,\"account_id\"\n",
    "                                             ,\"cast(amount as float)\"\n",
    "                                             ,\"cast(transaction_requested_at as bigint)\"\n",
    "                                             ,\"cast(transaction_completed_at as bigint)\"\n",
    "                                             ,\"status\"\n",
    "                              )\n",
    "df_transfer_outs.show(2,false)\n",
    "table = \"transfer_outs\"\n",
    "dir_path = warehouseLocation + \"/\" + data_base + \".db/\" + table\n",
    "db_table = data_base + \".\" + table\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + db_table)\n",
    "df_transfer_outs.toDF.write\n",
    "  .option(\"path\", dir_path)\n",
    "  .mode(\"Overwrite\")\n",
    "  .saveAsTable(db_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
